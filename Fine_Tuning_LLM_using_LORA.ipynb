{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26d750249e9640daa53d9f04562cf169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2148430ebff14720a06dcc3b2b473e44",
              "IPY_MODEL_fda1e81b0ee04b5098be73c7bbeae7fc",
              "IPY_MODEL_4be859a16c0b4edd897e101ebeeeffac"
            ],
            "layout": "IPY_MODEL_60804dc985d145249a20011bffdb2601"
          }
        },
        "2148430ebff14720a06dcc3b2b473e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7655909c2f940ec8033653ac6eccb54",
            "placeholder": "​",
            "style": "IPY_MODEL_6da72c23585b40e49b3e19411559ea51",
            "value": "Map: 100%"
          }
        },
        "fda1e81b0ee04b5098be73c7bbeae7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daea6c98925d471bb153422182b66a19",
            "max": 7773,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca865922560f482f8ab6a9b64008ace7",
            "value": 7773
          }
        },
        "4be859a16c0b4edd897e101ebeeeffac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8d58a9ebf1b43f2a09cb09e4520329a",
            "placeholder": "​",
            "style": "IPY_MODEL_611096e205bf49d991224eb75bc6da93",
            "value": " 7773/7773 [04:25&lt;00:00, 23.74 examples/s]"
          }
        },
        "60804dc985d145249a20011bffdb2601": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7655909c2f940ec8033653ac6eccb54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da72c23585b40e49b3e19411559ea51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daea6c98925d471bb153422182b66a19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca865922560f482f8ab6a9b64008ace7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8d58a9ebf1b43f2a09cb09e4520329a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "611096e205bf49d991224eb75bc6da93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb6RhSk0WFi8",
        "outputId": "d38b2a18-30fc-47b6-d68c-1c6e17d84bc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "PyTorch CUDA: 12.1\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!python -c \"import torch; print('PyTorch CUDA:', torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y torch torchvision torchaudio\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # CUDA 12.1 is closest available (12.5 not yet supported)"
      ],
      "metadata": {
        "id": "j9se-i_fWyYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers datasets evaluate rouge_score"
      ],
      "metadata": {
        "id": "TPHFhf8CXqmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zkDSFuWLhj5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model & tokenizer\n",
        "model_name = \"google-t5/t5-small\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWLsFn2HiUJx",
        "outputId": "4f883c14-8e1e-4083-8d70-a8cac269ad03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q\", \"v\", \"k\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9wyc55YijyR",
        "outputId": "db87d892-d97f-45e6-9d82-ba825d6653db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,769,472 || all params: 62,276,096 || trainable%: 2.8413\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset & tokenize\n",
        "dataset = load_dataset(\"AjayMukundS/Legal_Text_Summarization-llama2\")\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize inputs (encoder)\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Tokenize labels (decoder)\n",
        "    labels = tokenizer(\n",
        "        examples[\"summary\"],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_ds = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "26d750249e9640daa53d9f04562cf169",
            "2148430ebff14720a06dcc3b2b473e44",
            "fda1e81b0ee04b5098be73c7bbeae7fc",
            "4be859a16c0b4edd897e101ebeeeffac",
            "60804dc985d145249a20011bffdb2601",
            "c7655909c2f940ec8033653ac6eccb54",
            "6da72c23585b40e49b3e19411559ea51",
            "daea6c98925d471bb153422182b66a19",
            "ca865922560f482f8ab6a9b64008ace7",
            "e8d58a9ebf1b43f2a09cb09e4520329a",
            "611096e205bf49d991224eb75bc6da93"
          ]
        },
        "id": "o8sZWowRins-",
        "outputId": "8cbeb7d1-78ba-4bd7-c11b-a75beb18431e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7773 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26d750249e9640daa53d9f04562cf169"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for seq2seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "MGpDKva_isGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics (ROUGE)\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "5Kj_bZuMixWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"legal-t5-lora\",\n",
        "    run_name=\"legal-lora-t5-small\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=4,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "hrQYo0X2cXme",
        "outputId": "d58cae3c-5f66-43d7-ff69-a94c594afc99"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='678' max='1944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 678/1944 07:38 < 14:19, 1.47 it/s, Epoch 1.39/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.418691</td>\n",
              "      <td>0.011200</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.009200</td>\n",
              "      <td>4.700000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1944' max='1944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1944/1944 22:11, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.418691</td>\n",
              "      <td>0.011200</td>\n",
              "      <td>0.004400</td>\n",
              "      <td>0.009100</td>\n",
              "      <td>0.009200</td>\n",
              "      <td>4.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.708100</td>\n",
              "      <td>3.377537</td>\n",
              "      <td>0.015300</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>0.012900</td>\n",
              "      <td>0.012700</td>\n",
              "      <td>5.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.596800</td>\n",
              "      <td>3.358495</td>\n",
              "      <td>0.023600</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>0.019800</td>\n",
              "      <td>8.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.548800</td>\n",
              "      <td>3.352857</td>\n",
              "      <td>0.029100</td>\n",
              "      <td>0.013200</td>\n",
              "      <td>0.024500</td>\n",
              "      <td>0.024500</td>\n",
              "      <td>10.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1944, training_loss=3.5981584085849088, metrics={'train_runtime': 1332.3658, 'train_samples_per_second': 23.336, 'train_steps_per_second': 1.459, 'total_flos': 4377057744125952.0, 'train_loss': 3.5981584085849088, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"legal-t5-lora\")\n",
        "tokenizer.save_pretrained(\"legal-t5-lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6iAEsItpwZF",
        "outputId": "a75a8a99-eebb-4c8e-b5da-d8d18e3c3398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('legal-t5-lora/tokenizer_config.json',\n",
              " 'legal-t5-lora/special_tokens_map.json',\n",
              " 'legal-t5-lora/spiece.model',\n",
              " 'legal-t5-lora/added_tokens.json',\n",
              " 'legal-t5-lora/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
      ],
      "metadata": {
        "id": "AcbLqTHGoOUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=\"legal-t5-lora\")\n",
        "summarizer(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIzZTPQ_oSQM",
        "outputId": "e7546670-0dac-4cfd-925e-91e783d631ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Your max_length is set to 200, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': \"the inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs . it's the most aggressive action on tackling the climate crisis in American history . no one making under $400,000 per year will pay a penny more in taxes .\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "# Initialize\n",
        "nltk.download('punkt')\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"AjayMukundS/Legal_Text_Summarization-llama2\", split=\"test[:10%]\")\n",
        "\n",
        "# =============================================\n",
        "# 1. Evaluate Pre-trained Model (Baseline)\n",
        "# =============================================\n",
        "pretrained_model_name = \"google-t5/t5-small\"\n",
        "\n",
        "# Load with memory optimization\n",
        "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    pretrained_model_name,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "\n",
        "# Initialize ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def evaluate_model(model, tokenizer, dataset, max_samples=50):\n",
        "    scores = []\n",
        "    for i, example in enumerate(dataset):\n",
        "        if i >= max_samples:  # Limit samples to save memory\n",
        "            break\n",
        "\n",
        "        inputs = tokenizer(example[\"text\"],\n",
        "                         truncation=True,\n",
        "                         max_length=1024,\n",
        "                         return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=150,\n",
        "                min_length=50,\n",
        "                num_beams=4,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        ref = example[\"summary\"]\n",
        "\n",
        "        scores.append(scorer.score(ref, pred))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_scores = {\n",
        "        'rouge1': sum(s['rouge1'].fmeasure for s in scores) / len(scores),\n",
        "        'rouge2': sum(s['rouge2'].fmeasure for s in scores) / len(scores),\n",
        "        'rougeL': sum(s['rougeL'].fmeasure for s in scores) / len(scores)\n",
        "    }\n",
        "    return avg_scores\n",
        "\n",
        "print(\"Evaluating pre-trained model...\")\n",
        "pretrained_scores = evaluate_model(pretrained_model, tokenizer, dataset)\n",
        "print(f\"Pre-trained ROUGE: {pretrained_scores}\")\n",
        "\n",
        "# Clean up\n",
        "del pretrained_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# =============================================\n",
        "# 2. Evaluate Fine-tuned Model\n",
        "# =============================================\n",
        "finetuned_path = \"legal-t5-lora\"  # Your fine-tuned model path\n",
        "\n",
        "# Load with same optimizations\n",
        "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    finetuned_path,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_path)\n",
        "\n",
        "print(\"\\nEvaluating fine-tuned model...\")\n",
        "finetuned_scores = evaluate_model(finetuned_model, finetuned_tokenizer, dataset)\n",
        "print(f\"Fine-tuned ROUGE: {finetuned_scores}\")\n",
        "\n",
        "# =============================================\n",
        "# 3. Comparison Report\n",
        "# =============================================\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "    delta = finetuned_scores[metric] - pretrained_scores[metric]\n",
        "    print(f\"{metric.upper():<7}: {pretrained_scores[metric]:.3f} → {finetuned_scores[metric]:.3f} ({delta:+.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ssRAu1KqvNE",
        "outputId": "14e97707-fa9f-45bd-dc24-e5708ee80e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating pre-trained model...\n",
            "Pre-trained ROUGE: {'rouge1': 0.14013292474655317, 'rouge2': 0.05397039884115993, 'rougeL': 0.09792356565974421}\n",
            "\n",
            "Evaluating fine-tuned model...\n",
            "Fine-tuned ROUGE: {'rouge1': 0.19945795550302123, 'rouge2': 0.09179483394193164, 'rougeL': 0.1272012410494623}\n",
            "\n",
            "Performance Comparison:\n",
            "ROUGE1 : 0.140 → 0.199 (+0.059)\n",
            "ROUGE2 : 0.054 → 0.092 (+0.038)\n",
            "ROUGEL : 0.098 → 0.127 (+0.029)\n"
          ]
        }
      ]
    }
  ]
}